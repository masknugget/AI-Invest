# TradingAgents/graph/trading_graph.py

import os
from pathlib import Path
import json
from datetime import date
from typing import Dict, Any, Tuple, List, Optional
import time

from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_google_genai import ChatGoogleGenerativeAI


from langgraph.prebuilt import ToolNode

from tradingagents.agents import *
from tradingagents.default_config import DEFAULT_CONFIG
from tradingagents.agents.utils.memory import FinancialSituationMemory

# å¯¼å…¥ç»Ÿä¸€æ—¥å¿—ç³»ç»Ÿ
from tradingagents.utils.logging_init import get_logger

# å¯¼å…¥æ—¥å¿—æ¨¡å—
from tradingagents.utils.logging_manager import get_logger
from ..llm_adapters.internal_adapter import InternalLLM

logger = get_logger('agents')
from tradingagents.agents.utils.agent_states import (
    AgentState,
    InvestDebateState,
    RiskDebateState,
)
from tradingagents.dataflows.interface import set_config

from .conditional_logic import ConditionalLogic
from .setup import GraphSetup
from .propagation import Propagator
from .reflection import Reflector
from .signal_processing import SignalProcessor


class TradingAgentsGraph:
    """Main class that orchestrates the trading agents framework."""

    def __init__(
        self,
        selected_analysts=["market", "social", "news", "fundamentals"],
        debug=False,
        config: Dict[str, Any] = None,
    ):
        """Initialize the trading agents graph and components.

        Args:
            selected_analysts: List of analyst types to include
            debug: Whether to run in debug mode
            config: Configuration dictionary. If None, uses default config
        """
        self.debug = debug
        self.config = config or DEFAULT_CONFIG

        # Update the interface's config
        set_config(self.config)

        # Create necessary directories
        os.makedirs(
            os.path.join(self.config["project_dir"], "dataflows/data_cache"),
            exist_ok=True,
        )

        # Initialize LLMs
        # ğŸ”§ ä»é…ç½®ä¸­è¯»å–æ¨¡å‹å‚æ•°ï¼ˆä¼˜å…ˆä½¿ç”¨ç”¨æˆ·é…ç½®ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤å€¼ï¼‰
        quick_config = self.config.get("quick_model_config", {})
        deep_config = self.config.get("deep_model_config", {})

        # è¯»å–å¿«é€Ÿæ¨¡å‹å‚æ•°
        quick_max_tokens = quick_config.get("max_tokens", 4000)
        quick_temperature = quick_config.get("temperature", 0.7)
        quick_timeout = quick_config.get("timeout", 180)

        # è¯»å–æ·±åº¦æ¨¡å‹å‚æ•°
        deep_max_tokens = deep_config.get("max_tokens", 4000)
        deep_temperature = deep_config.get("temperature", 0.7)
        deep_timeout = deep_config.get("timeout", 180)

        # ğŸ”§ æ£€æŸ¥æ˜¯å¦ä¸ºæ··åˆæ¨¡å¼ï¼ˆå¿«é€Ÿæ¨¡å‹å’Œæ·±åº¦æ¨¡å‹æ¥è‡ªä¸åŒå‚å®¶ï¼‰
        quick_provider = self.config.get("quick_provider")
        deep_provider = self.config.get("deep_provider")
        quick_backend_url = self.config.get("quick_backend_url")
        deep_backend_url = self.config.get("deep_backend_url")

        # ä½¿ç”¨ç»Ÿä¸€çš„å‡½æ•°åˆ›å»º LLM å®ä¾‹
        self.quick_thinking_llm = InternalLLM()

        self.deep_thinking_llm = InternalLLM()
        
        self.toolkit = Toolkit(config=self.config)

        # Initialize memories (å¦‚æœå¯ç”¨)
        memory_enabled = self.config.get("memory_enabled", True)
        if memory_enabled:
            # ä½¿ç”¨å•ä¾‹ChromaDBç®¡ç†å™¨ï¼Œé¿å…å¹¶å‘åˆ›å»ºå†²çª
            self.bull_memory = FinancialSituationMemory("bull_memory", self.config)
            self.bear_memory = FinancialSituationMemory("bear_memory", self.config)
            self.trader_memory = FinancialSituationMemory("trader_memory", self.config)
            self.invest_judge_memory = FinancialSituationMemory("invest_judge_memory", self.config)
            self.risk_manager_memory = FinancialSituationMemory("risk_manager_memory", self.config)
        else:
            # åˆ›å»ºç©ºçš„å†…å­˜å¯¹è±¡
            self.bull_memory = None
            self.bear_memory = None
            self.trader_memory = None
            self.invest_judge_memory = None
            self.risk_manager_memory = None

        # Create tool nodes
        self.tool_nodes = self._create_tool_nodes()

        # Initialize components
        # ğŸ”¥ [ä¿®å¤] ä»é…ç½®ä¸­è¯»å–è¾©è®ºè½®æ¬¡å‚æ•°
        self.conditional_logic = ConditionalLogic(
            max_debate_rounds=self.config.get("max_debate_rounds", 1),
            max_risk_discuss_rounds=self.config.get("max_risk_discuss_rounds", 1)
        )
        logger.info(f"ğŸ”§ [ConditionalLogic] åˆå§‹åŒ–å®Œæˆ:")
        logger.info(f"   - max_debate_rounds: {self.conditional_logic.max_debate_rounds}")
        logger.info(f"   - max_risk_discuss_rounds: {self.conditional_logic.max_risk_discuss_rounds}")

        self.graph_setup = GraphSetup(
            self.quick_thinking_llm,
            self.deep_thinking_llm,
            self.toolkit,
            self.tool_nodes,
            self.bull_memory,
            self.bear_memory,
            self.trader_memory,
            self.invest_judge_memory,
            self.risk_manager_memory,
            self.conditional_logic,
            self.config,
            getattr(self, 'react_llm', None),
        )

        self.propagator = Propagator()
        self.reflector = Reflector(self.quick_thinking_llm)
        self.signal_processor = SignalProcessor(self.quick_thinking_llm)

        # State tracking
        self.curr_state = None
        self.ticker = None
        self.log_states_dict = {}  # date to full state dict

        # Set up the graph
        self.graph = self.graph_setup.setup_graph(selected_analysts)

    def _create_tool_nodes(self) -> Dict[str, ToolNode]:
        """Create tool nodes for different data sources.

        æ³¨æ„ï¼šToolNode åŒ…å«æ‰€æœ‰å¯èƒ½çš„å·¥å…·ï¼Œä½† LLM åªä¼šè°ƒç”¨å®ƒç»‘å®šçš„å·¥å…·ã€‚
        ToolNode çš„ä½œç”¨æ˜¯æ‰§è¡Œ LLM ç”Ÿæˆçš„ tool_callsï¼Œè€Œä¸æ˜¯é™åˆ¶ LLM å¯ä»¥è°ƒç”¨å“ªäº›å·¥å…·ã€‚
        """
        return {
            "market": ToolNode(
                [
                    # ç»Ÿä¸€å·¥å…·ï¼ˆæ¨èï¼‰
                    self.toolkit.get_stock_market_data_unified,
                ]
            ),
            "social": ToolNode(
                [
                    # ç»Ÿä¸€å·¥å…·ï¼ˆæ¨èï¼‰
                    self.toolkit.get_stock_sentiment_unified,

                ]
            ),
            "news": ToolNode(
                [
                    # ç»Ÿä¸€å·¥å…·ï¼ˆæ¨èï¼‰
                    self.toolkit.get_stock_news_unified,

                ]
            ),
            "fundamentals": ToolNode(
                [
                    # ç»Ÿä¸€å·¥å…·ï¼ˆæ¨èï¼‰
                    self.toolkit.get_stock_fundamentals_unified,
                ]
            ),
        }

    def propagate(self, company_name, trade_date, language="zh-CN", progress_callback=None, task_id=None):
        """Run the trading agents graph for a company on a specific date.

        Args:
            company_name: Company name or stock symbol
            trade_date: Date for analysis
            progress_callback: Optional callback function for progress updates
            language: Optional callback function for progress updates
            task_id: Optional task ID for tracking performance data
        """

        # æ·»åŠ è¯¦ç»†çš„æ¥æ”¶æ—¥å¿—
        self.ticker = company_name
        logger.debug(f"ğŸ” [GRAPH DEBUG] è®¾ç½®self.ticker: '{self.ticker}'")

        # Initialize state
        logger.debug(f"ğŸ” [GRAPH DEBUG] åˆ›å»ºåˆå§‹çŠ¶æ€ï¼Œä¼ é€’å‚æ•°: company_name='{company_name}', trade_date='{trade_date}'")
        init_agent_state = self.propagator.create_initial_state(
            company_name, trade_date, language
        )
        logger.debug(f"ğŸ” [GRAPH DEBUG] åˆå§‹çŠ¶æ€ä¸­çš„company_of_interest: '{init_agent_state.get('company_of_interest', 'NOT_FOUND')}'")
        logger.debug(f"ğŸ” [GRAPH DEBUG] åˆå§‹çŠ¶æ€ä¸­çš„trade_date: '{init_agent_state.get('trade_date', 'NOT_FOUND')}'")

        # åˆå§‹åŒ–è®¡æ—¶å™¨
        node_timings = {}  # è®°å½•æ¯ä¸ªèŠ‚ç‚¹çš„æ‰§è¡Œæ—¶é—´
        total_start_time = time.time()  # æ€»ä½“å¼€å§‹æ—¶é—´
        current_node_start = None  # å½“å‰èŠ‚ç‚¹å¼€å§‹æ—¶é—´
        current_node_name = None  # å½“å‰èŠ‚ç‚¹åç§°

        # ä¿å­˜task_idç”¨äºåç»­ä¿å­˜æ€§èƒ½æ•°æ®
        self._current_task_id = task_id

        # æ ¹æ®æ˜¯å¦æœ‰è¿›åº¦å›è°ƒé€‰æ‹©ä¸åŒçš„stream_mode
        args = self.propagator.get_graph_args(use_progress_callback=bool(progress_callback))


        # Standard mode without tracing but with progress updates
        if progress_callback:
            # ä½¿ç”¨ updates æ¨¡å¼ä»¥ä¾¿è·å–èŠ‚ç‚¹çº§åˆ«çš„è¿›åº¦
            trace = []
            final_state = None
            for chunk in self.graph.stream(init_agent_state, **args):
                # è®°å½•èŠ‚ç‚¹è®¡æ—¶
                for node_name in chunk.keys():
                    if not node_name.startswith('__'):
                        # å¦‚æœæœ‰ä¸Šä¸€ä¸ªèŠ‚ç‚¹ï¼Œè®°å½•å…¶ç»“æŸæ—¶é—´
                        if current_node_name and current_node_start:
                            elapsed = time.time() - current_node_start
                            node_timings[current_node_name] = elapsed
                            logger.info(f"â±ï¸ [{current_node_name}] è€—æ—¶: {elapsed:.2f}ç§’")
                            logger.info(f"ğŸ” [TIMING] èŠ‚ç‚¹åˆ‡æ¢: {current_node_name} â†’ {node_name}")

                        # å¼€å§‹æ–°èŠ‚ç‚¹è®¡æ—¶
                        current_node_name = node_name
                        current_node_start = time.time()
                        logger.info(f"ğŸ” [TIMING] å¼€å§‹è®¡æ—¶: {node_name}")
                        break

                self._send_progress_update(chunk, progress_callback)
                # ç´¯ç§¯çŠ¶æ€æ›´æ–°
                if final_state is None:
                    final_state = init_agent_state.copy()
                for node_name, node_update in chunk.items():
                    if not node_name.startswith('__'):
                        final_state.update(node_update)
        else:
            # åŸæœ‰çš„invokeæ¨¡å¼ï¼ˆä¹Ÿéœ€è¦è®¡æ—¶ï¼‰
            logger.info("â±ï¸ ä½¿ç”¨ invoke æ¨¡å¼æ‰§è¡Œåˆ†æï¼ˆæ— è¿›åº¦å›è°ƒï¼‰")
            # ä½¿ç”¨streamæ¨¡å¼ä»¥ä¾¿è®¡æ—¶ï¼Œä½†ä¸å‘é€è¿›åº¦æ›´æ–°
            trace = []
            final_state = None
            for chunk in self.graph.stream(init_agent_state, **args):
                # è®°å½•èŠ‚ç‚¹è®¡æ—¶
                for node_name in chunk.keys():
                    if not node_name.startswith('__'):
                        # å¦‚æœæœ‰ä¸Šä¸€ä¸ªèŠ‚ç‚¹ï¼Œè®°å½•å…¶ç»“æŸæ—¶é—´
                        if current_node_name and current_node_start:
                            elapsed = time.time() - current_node_start
                            node_timings[current_node_name] = elapsed
                            logger.info(f"â±ï¸ [{current_node_name}] è€—æ—¶: {elapsed:.2f}ç§’")

                        # å¼€å§‹æ–°èŠ‚ç‚¹è®¡æ—¶
                        current_node_name = node_name
                        current_node_start = time.time()
                        break

                # ç´¯ç§¯çŠ¶æ€æ›´æ–°
                if final_state is None:
                    final_state = init_agent_state.copy()
                for node_name, node_update in chunk.items():
                    if not node_name.startswith('__'):
                        final_state.update({node_name: node_update})

        # è®°å½•æœ€åä¸€ä¸ªèŠ‚ç‚¹çš„æ—¶é—´
        if current_node_name and current_node_start:
            elapsed = time.time() - current_node_start
            node_timings[current_node_name] = elapsed
            logger.info(f"â±ï¸ [{current_node_name}] è€—æ—¶: {elapsed:.2f}ç§’")

        # è®¡ç®—æ€»æ—¶é—´
        # total_elapsed = time.time() - total_start_time
        #
        # # è°ƒè¯•æ—¥å¿—
        # logger.info(f"ğŸ” [TIMING DEBUG] èŠ‚ç‚¹è®¡æ—¶æ•°é‡: {len(node_timings)}")
        # logger.info(f"ğŸ” [TIMING DEBUG] æ€»è€—æ—¶: {total_elapsed:.2f}ç§’")
        # logger.info(f"ğŸ” [TIMING DEBUG] èŠ‚ç‚¹åˆ—è¡¨: {list(node_timings.keys())}")
        #
        # # æ‰“å°è¯¦ç»†çš„æ—¶é—´ç»Ÿè®¡
        # logger.info("ğŸ” [TIMING DEBUG] å‡†å¤‡è°ƒç”¨ _print_timing_summary")
        # self._print_timing_summary(node_timings, total_elapsed)
        # logger.info("ğŸ” [TIMING DEBUG] _print_timing_summary è°ƒç”¨å®Œæˆ")
        #
        # # æ„å»ºæ€§èƒ½æ•°æ®
        # performance_data = self._build_performance_data(node_timings, total_elapsed)
        #
        # # å°†æ€§èƒ½æ•°æ®æ·»åŠ åˆ°çŠ¶æ€ä¸­
        # final_state['performance_metrics'] = performance_data
        #
        # # Store current state for reflection
        # self.curr_state = final_state
        #
        # # Log state
        # # self._log_state(trade_date, final_state)
        #
        # # è·å–æ¨¡å‹ä¿¡æ¯
        # model_info = ""
        # try:
        #     if hasattr(self.deep_thinking_llm, 'model_name'):
        #         model_info = f"{self.deep_thinking_llm.__class__.__name__}:{self.deep_thinking_llm.model_name}"
        #     else:
        #         model_info = self.deep_thinking_llm.__class__.__name__
        # except Exception:
        #     model_info = "Unknown"
        #
        # # å¤„ç†å†³ç­–å¹¶æ·»åŠ æ¨¡å‹ä¿¡æ¯
        decision = self.process_signal(final_state["final_trade_decision"], company_name, language)
        # decision['model_info'] = model_info

        # Return decision and processed signal
        # return final_state, decision
        if isinstance(final_state, dict):
            if 'investment_debate_state' in final_state:
                del final_state['investment_debate_state']
            if 'risk_debate_state' in final_state:
                del final_state['risk_debate_state']
        return final_state, decision

    def _send_progress_update(self, chunk, progress_callback):
        """å‘é€è¿›åº¦æ›´æ–°åˆ°å›è°ƒå‡½æ•°

        LangGraph stream è¿”å›çš„ chunk æ ¼å¼ï¼š{node_name: {...}}
        èŠ‚ç‚¹åç§°ç¤ºä¾‹ï¼š
        - "Market Analyst", "Fundamentals Analyst", "News Analyst", "Social Analyst"
        - "tools_market", "tools_fundamentals", "tools_news", "tools_social"
        - "Msg Clear Market", "Msg Clear Fundamentals", etc.
        - "Bull Researcher", "Bear Researcher", "Research Manager"
        - "Trader"
        - "Risky Analyst", "Safe Analyst", "Neutral Analyst", "Risk Judge"
        """
        try:
            # ä»chunkä¸­æå–å½“å‰æ‰§è¡Œçš„èŠ‚ç‚¹ä¿¡æ¯
            if not isinstance(chunk, dict):
                return

            # è·å–ç¬¬ä¸€ä¸ªéç‰¹æ®Šé”®ä½œä¸ºèŠ‚ç‚¹å
            node_name = None
            for key in chunk.keys():
                if not key.startswith('__'):
                    node_name = key
                    break

            if not node_name:
                return

            logger.info(f"ğŸ” [Progress] èŠ‚ç‚¹åç§°: {node_name}")

            # æ£€æŸ¥æ˜¯å¦ä¸ºç»“æŸèŠ‚ç‚¹
            if '__end__' in chunk:
                logger.info(f"ğŸ“Š [Progress] æ£€æµ‹åˆ°__end__èŠ‚ç‚¹")
                progress_callback("ğŸ“Š ç”ŸæˆæŠ¥å‘Š")
                return

            # èŠ‚ç‚¹åç§°æ˜ å°„è¡¨ï¼ˆåŒ¹é… LangGraph å®é™…èŠ‚ç‚¹åï¼‰
            node_mapping = {
                # åˆ†æå¸ˆèŠ‚ç‚¹
                'Market Analyst': "ğŸ“Š å¸‚åœºåˆ†æå¸ˆ",
                'Fundamentals Analyst': "ğŸ’¼ åŸºæœ¬é¢åˆ†æå¸ˆ",
                'News Analyst': "ğŸ“° æ–°é—»åˆ†æå¸ˆ",
                'Social Analyst': "ğŸ’¬ ç¤¾äº¤åª’ä½“åˆ†æå¸ˆ",
                # å·¥å…·èŠ‚ç‚¹ï¼ˆä¸å‘é€è¿›åº¦æ›´æ–°ï¼Œé¿å…é‡å¤ï¼‰
                'tools_market': None,
                'tools_fundamentals': None,
                'tools_news': None,
                'tools_social': None,
                # æ¶ˆæ¯æ¸…ç†èŠ‚ç‚¹ï¼ˆä¸å‘é€è¿›åº¦æ›´æ–°ï¼‰
                'Msg Clear Market': None,
                'Msg Clear Fundamentals': None,
                'Msg Clear News': None,
                'Msg Clear Social': None,
                # ç ”ç©¶å‘˜èŠ‚ç‚¹
                'Bull Researcher': "ğŸ‚ çœ‹æ¶¨ç ”ç©¶å‘˜",
                'Bear Researcher': "ğŸ» çœ‹è·Œç ”ç©¶å‘˜",
                'Research Manager': "ğŸ‘” ç ”ç©¶ç»ç†",
                # äº¤æ˜“å‘˜èŠ‚ç‚¹
                'Trader': "ğŸ’¼ äº¤æ˜“å‘˜å†³ç­–",
                # é£é™©è¯„ä¼°èŠ‚ç‚¹
                'Risky Analyst': "ğŸ”¥ æ¿€è¿›é£é™©è¯„ä¼°",
                'Safe Analyst': "ğŸ›¡ï¸ ä¿å®ˆé£é™©è¯„ä¼°",
                'Neutral Analyst': "âš–ï¸ ä¸­æ€§é£é™©è¯„ä¼°",
                'Risk Judge': "ğŸ¯ é£é™©ç»ç†",
            }

            # æŸ¥æ‰¾æ˜ å°„çš„æ¶ˆæ¯
            message = node_mapping.get(node_name)

            if message is None:
                # None è¡¨ç¤ºè·³è¿‡ï¼ˆå·¥å…·èŠ‚ç‚¹ã€æ¶ˆæ¯æ¸…ç†èŠ‚ç‚¹ï¼‰
                logger.debug(f"â­ï¸ [Progress] è·³è¿‡èŠ‚ç‚¹: {node_name}")
                return

            if message:
                # å‘é€è¿›åº¦æ›´æ–°
                logger.info(f"ğŸ“¤ [Progress] å‘é€è¿›åº¦æ›´æ–°: {message}")
                progress_callback(message)
            else:
                # æœªçŸ¥èŠ‚ç‚¹ï¼Œä½¿ç”¨èŠ‚ç‚¹åç§°
                logger.warning(f"âš ï¸ [Progress] æœªçŸ¥èŠ‚ç‚¹: {node_name}")
                progress_callback(f"ğŸ” {node_name}")

        except Exception as e:
            logger.error(f"âŒ è¿›åº¦æ›´æ–°å¤±è´¥: {e}", exc_info=True)

    def _build_performance_data(self, node_timings: Dict[str, float], total_elapsed: float) -> Dict[str, Any]:
        """æ„å»ºæ€§èƒ½æ•°æ®ç»“æ„

        Args:
            node_timings: æ¯ä¸ªèŠ‚ç‚¹çš„æ‰§è¡Œæ—¶é—´å­—å…¸
            total_elapsed: æ€»æ‰§è¡Œæ—¶é—´

        Returns:
            æ€§èƒ½æ•°æ®å­—å…¸
        """
        # èŠ‚ç‚¹åˆ†ç±»ï¼ˆæ³¨æ„ï¼šé£é™©ç®¡ç†èŠ‚ç‚¹è¦å…ˆäºåˆ†æå¸ˆèŠ‚ç‚¹åˆ¤æ–­ï¼Œå› ä¸ºå®ƒä»¬ä¹ŸåŒ…å«'Analyst'ï¼‰
        analyst_nodes = {}
        tool_nodes = {}
        msg_clear_nodes = {}
        research_nodes = {}
        trader_nodes = {}
        risk_nodes = {}
        other_nodes = {}

        for node_name, elapsed in node_timings.items():
            # ä¼˜å…ˆåŒ¹é…é£é™©ç®¡ç†å›¢é˜Ÿï¼ˆå› ä¸ºå®ƒä»¬ä¹ŸåŒ…å«'Analyst'ï¼‰
            if 'Risky' in node_name or 'Safe' in node_name or 'Neutral' in node_name or 'Risk Judge' in node_name:
                risk_nodes[node_name] = elapsed
            # ç„¶ååŒ¹é…åˆ†æå¸ˆå›¢é˜Ÿ
            elif 'Analyst' in node_name:
                analyst_nodes[node_name] = elapsed
            # å·¥å…·èŠ‚ç‚¹
            elif node_name.startswith('tools_'):
                tool_nodes[node_name] = elapsed
            # æ¶ˆæ¯æ¸…ç†èŠ‚ç‚¹
            elif node_name.startswith('Msg Clear'):
                msg_clear_nodes[node_name] = elapsed
            # ç ”ç©¶å›¢é˜Ÿ
            elif 'Researcher' in node_name or 'Research Manager' in node_name:
                research_nodes[node_name] = elapsed
            # äº¤æ˜“å›¢é˜Ÿ
            elif 'Trader' in node_name:
                trader_nodes[node_name] = elapsed
            # å…¶ä»–èŠ‚ç‚¹
            else:
                other_nodes[node_name] = elapsed

        # è®¡ç®—ç»Ÿè®¡æ•°æ®
        slowest_node = max(node_timings.items(), key=lambda x: x[1]) if node_timings else (None, 0)
        fastest_node = min(node_timings.items(), key=lambda x: x[1]) if node_timings else (None, 0)
        avg_time = sum(node_timings.values()) / len(node_timings) if node_timings else 0

        return {
            "total_time": round(total_elapsed, 2),
            "total_time_minutes": round(total_elapsed / 60, 2),
            "node_count": len(node_timings),
            "average_node_time": round(avg_time, 2),
            "slowest_node": {
                "name": slowest_node[0],
                "time": round(slowest_node[1], 2)
            } if slowest_node[0] else None,
            "fastest_node": {
                "name": fastest_node[0],
                "time": round(fastest_node[1], 2)
            } if fastest_node[0] else None,
            "node_timings": {k: round(v, 2) for k, v in node_timings.items()},
            "category_timings": {
                "analyst_team": {
                    "nodes": {k: round(v, 2) for k, v in analyst_nodes.items()},
                    "total": round(sum(analyst_nodes.values()), 2),
                    "percentage": round(sum(analyst_nodes.values()) / total_elapsed * 100, 1) if total_elapsed > 0 else 0
                },
                "tool_calls": {
                    "nodes": {k: round(v, 2) for k, v in tool_nodes.items()},
                    "total": round(sum(tool_nodes.values()), 2),
                    "percentage": round(sum(tool_nodes.values()) / total_elapsed * 100, 1) if total_elapsed > 0 else 0
                },
                "message_clearing": {
                    "nodes": {k: round(v, 2) for k, v in msg_clear_nodes.items()},
                    "total": round(sum(msg_clear_nodes.values()), 2),
                    "percentage": round(sum(msg_clear_nodes.values()) / total_elapsed * 100, 1) if total_elapsed > 0 else 0
                },
                "research_team": {
                    "nodes": {k: round(v, 2) for k, v in research_nodes.items()},
                    "total": round(sum(research_nodes.values()), 2),
                    "percentage": round(sum(research_nodes.values()) / total_elapsed * 100, 1) if total_elapsed > 0 else 0
                },
                "trader_team": {
                    "nodes": {k: round(v, 2) for k, v in trader_nodes.items()},
                    "total": round(sum(trader_nodes.values()), 2),
                    "percentage": round(sum(trader_nodes.values()) / total_elapsed * 100, 1) if total_elapsed > 0 else 0
                },
                "risk_management_team": {
                    "nodes": {k: round(v, 2) for k, v in risk_nodes.items()},
                    "total": round(sum(risk_nodes.values()), 2),
                    "percentage": round(sum(risk_nodes.values()) / total_elapsed * 100, 1) if total_elapsed > 0 else 0
                },
                "other": {
                    "nodes": {k: round(v, 2) for k, v in other_nodes.items()},
                    "total": round(sum(other_nodes.values()), 2),
                    "percentage": round(sum(other_nodes.values()) / total_elapsed * 100, 1) if total_elapsed > 0 else 0
                }
            },
            "llm_config": {
                "provider": self.config.get('llm_provider', 'unknown'),
                "deep_think_model": self.config.get('deep_think_llm', 'unknown'),
                "quick_think_model": self.config.get('quick_think_llm', 'unknown')
            }
        }

    def _print_timing_summary(self, node_timings: Dict[str, float], total_elapsed: float):
        """æ‰“å°è¯¦ç»†çš„æ—¶é—´ç»Ÿè®¡æŠ¥å‘Š

        Args:
            node_timings: æ¯ä¸ªèŠ‚ç‚¹çš„æ‰§è¡Œæ—¶é—´å­—å…¸
            total_elapsed: æ€»æ‰§è¡Œæ—¶é—´
        """
        logger.info("ğŸ” [_print_timing_summary] æ–¹æ³•è¢«è°ƒç”¨")
        logger.info("ğŸ” [_print_timing_summary] node_timings æ•°é‡: " + str(len(node_timings)))
        logger.info("ğŸ” [_print_timing_summary] total_elapsed: " + str(total_elapsed))

        logger.info("=" * 80)
        logger.info("â±ï¸  åˆ†ææ€§èƒ½ç»Ÿè®¡æŠ¥å‘Š")
        logger.info("=" * 80)

        # èŠ‚ç‚¹åˆ†ç±»ï¼ˆæ³¨æ„ï¼šé£é™©ç®¡ç†èŠ‚ç‚¹è¦å…ˆäºåˆ†æå¸ˆèŠ‚ç‚¹åˆ¤æ–­ï¼Œå› ä¸ºå®ƒä»¬ä¹ŸåŒ…å«'Analyst'ï¼‰
        analyst_nodes = []
        tool_nodes = []
        msg_clear_nodes = []
        research_nodes = []
        trader_nodes = []
        risk_nodes = []
        other_nodes = []

        for node_name, elapsed in node_timings.items():
            # ä¼˜å…ˆåŒ¹é…é£é™©ç®¡ç†å›¢é˜Ÿï¼ˆå› ä¸ºå®ƒä»¬ä¹ŸåŒ…å«'Analyst'ï¼‰
            if 'Risky' in node_name or 'Safe' in node_name or 'Neutral' in node_name or 'Risk Judge' in node_name:
                risk_nodes.append((node_name, elapsed))
            # ç„¶ååŒ¹é…åˆ†æå¸ˆå›¢é˜Ÿ
            elif 'Analyst' in node_name:
                analyst_nodes.append((node_name, elapsed))
            # å·¥å…·èŠ‚ç‚¹
            elif node_name.startswith('tools_'):
                tool_nodes.append((node_name, elapsed))
            # æ¶ˆæ¯æ¸…ç†èŠ‚ç‚¹
            elif node_name.startswith('Msg Clear'):
                msg_clear_nodes.append((node_name, elapsed))
            # ç ”ç©¶å›¢é˜Ÿ
            elif 'Researcher' in node_name or 'Research Manager' in node_name:
                research_nodes.append((node_name, elapsed))
            # äº¤æ˜“å›¢é˜Ÿ
            elif 'Trader' in node_name:
                trader_nodes.append((node_name, elapsed))
            # å…¶ä»–èŠ‚ç‚¹
            else:
                other_nodes.append((node_name, elapsed))

        # æ‰“å°åˆ†ç±»ç»Ÿè®¡
        def print_category(title: str, nodes: List[Tuple[str, float]]):
            if not nodes:
                return
            logger.info(f"\nğŸ“Š {title}")
            logger.info("-" * 80)
            total_category_time = sum(t for _, t in nodes)
            for node_name, elapsed in sorted(nodes, key=lambda x: x[1], reverse=True):
                percentage = (elapsed / total_elapsed * 100) if total_elapsed > 0 else 0
                logger.info(f"  â€¢ {node_name:40s} {elapsed:8.2f}ç§’  ({percentage:5.1f}%)")
            logger.info(f"  {'å°è®¡':40s} {total_category_time:8.2f}ç§’  ({total_category_time/total_elapsed*100:5.1f}%)")

        print_category("åˆ†æå¸ˆå›¢é˜Ÿ", analyst_nodes)
        print_category("å·¥å…·è°ƒç”¨", tool_nodes)
        print_category("æ¶ˆæ¯æ¸…ç†", msg_clear_nodes)
        print_category("ç ”ç©¶å›¢é˜Ÿ", research_nodes)
        print_category("äº¤æ˜“å›¢é˜Ÿ", trader_nodes)
        print_category("é£é™©ç®¡ç†å›¢é˜Ÿ", risk_nodes)
        print_category("å…¶ä»–èŠ‚ç‚¹", other_nodes)

        # æ‰“å°æ€»ä½“ç»Ÿè®¡
        logger.info("\n" + "=" * 80)
        logger.info(f"ğŸ¯ æ€»æ‰§è¡Œæ—¶é—´: {total_elapsed:.2f}ç§’ ({total_elapsed/60:.2f}åˆ†é’Ÿ)")
        logger.info(f"ğŸ“ˆ èŠ‚ç‚¹æ€»æ•°: {len(node_timings)}")
        if node_timings:
            avg_time = sum(node_timings.values()) / len(node_timings)
            logger.info(f"â±ï¸  å¹³å‡èŠ‚ç‚¹è€—æ—¶: {avg_time:.2f}ç§’")
            slowest_node = max(node_timings.items(), key=lambda x: x[1])
            logger.info(f"ğŸŒ æœ€æ…¢èŠ‚ç‚¹: {slowest_node[0]} ({slowest_node[1]:.2f}ç§’)")
            fastest_node = min(node_timings.items(), key=lambda x: x[1])
            logger.info(f"âš¡ æœ€å¿«èŠ‚ç‚¹: {fastest_node[0]} ({fastest_node[1]:.2f}ç§’)")

        # æ‰“å°LLMé…ç½®ä¿¡æ¯
        logger.info(f"\nğŸ¤– LLMé…ç½®:")
        logger.info(f"  â€¢ æä¾›å•†: {self.config.get('llm_provider', 'unknown')}")
        logger.info(f"  â€¢ æ·±åº¦æ€è€ƒæ¨¡å‹: {self.config.get('deep_think_llm', 'unknown')}")
        logger.info(f"  â€¢ å¿«é€Ÿæ€è€ƒæ¨¡å‹: {self.config.get('quick_think_llm', 'unknown')}")
        logger.info("=" * 80)

    def _log_state(self, trade_date, final_state):
        """Log the final state to a JSON file."""
        self.log_states_dict[str(trade_date)] = {
            "company_of_interest": final_state["company_of_interest"],
            "trade_date": final_state["trade_date"],
            "market_report": final_state["market_report"],
            "sentiment_report": final_state["sentiment_report"],
            "news_report": final_state["news_report"],
            "fundamentals_report": final_state["fundamentals_report"],
            "investment_debate_state": {
                "bull_history": final_state["investment_debate_state"]["bull_history"],
                "bear_history": final_state["investment_debate_state"]["bear_history"],
                "history": final_state["investment_debate_state"]["history"],
                "current_response": final_state["investment_debate_state"][
                    "current_response"
                ],
                "judge_decision": final_state["investment_debate_state"][
                    "judge_decision"
                ],
            },
            "trader_investment_decision": final_state["trader_investment_plan"],
            "risk_debate_state": {
                "risky_history": final_state["risk_debate_state"]["risky_history"],
                "safe_history": final_state["risk_debate_state"]["safe_history"],
                "neutral_history": final_state["risk_debate_state"]["neutral_history"],
                "history": final_state["risk_debate_state"]["history"],
                "judge_decision": final_state["risk_debate_state"]["judge_decision"],
            },
            "investment_plan": final_state["investment_plan"],
            "final_trade_decision": final_state["final_trade_decision"],
        }

        # Save to file
        directory = Path(f"eval_results/{self.ticker}/TradingAgentsStrategy_logs/")
        directory.mkdir(parents=True, exist_ok=True)

        with open(
            f"eval_results/{self.ticker}/TradingAgentsStrategy_logs/full_states_log.json",
            "w",
        ) as f:
            json.dump(self.log_states_dict, f, indent=4)

    def reflect_and_remember(self, returns_losses):
        """Reflect on decisions and update memory based on returns."""
        self.reflector.reflect_bull_researcher(
            self.curr_state, returns_losses, self.bull_memory
        )
        self.reflector.reflect_bear_researcher(
            self.curr_state, returns_losses, self.bear_memory
        )
        self.reflector.reflect_trader(
            self.curr_state, returns_losses, self.trader_memory
        )
        self.reflector.reflect_invest_judge(
            self.curr_state, returns_losses, self.invest_judge_memory
        )
        self.reflector.reflect_risk_manager(
            self.curr_state, returns_losses, self.risk_manager_memory
        )

    def process_signal(self, full_signal, stock_symbol=None, language='en-US'):
        """Process a signal to extract the core decision."""
        return self.signal_processor.process_signal(full_signal, stock_symbol, language)
